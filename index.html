<!DOCTYPE html>
<html>
<body>

<h1>Introduction and Project Overview </h1>
<p>    This page is dedicated to explaining what I have done and learned through Beaver Works’ 2016 summer program on autonomous vehicles. Beaver Works is a program that is run jointly through the Massachusetts Institute of Technology’s Engineering School and the nearby Lincoln Laboratory. Beaver Works focuses on creating project based learning and educational programs. This year was the first year that Beaver Works provided a summer program for rising high school seniors.
    As previously mentioned, this year’s Beaver Works’ Summer Program focused on the the programming and software that goes into making an autonomous vehicle. However, in addition to learning how to code an autonomous vehicle, we received daily presentations from guest speakers including people from MIT, Lincoln Laboratory, NASA’s Jet Propulsion Laboratory and an autonomous car manufacturing company in Germany. The subjects of these presentations ranged throughout all of the sciences and engineering disciplines, not just computer science and robotics. On a less technical side, we also attended a communications class twice a week for an hour in order to learn ways to be a more effective team member.
    The program spanned a four week period from July 11th to August 5th. However, there was a lot of pre program material to study so that students could come in with the necessary background knowledge to immediately start programming the race car.
    A normal day consisted of beginning at 9 AM with a technical lecture that related to the labs for the day and/or challenge for the week. These lectures would normally last about an hour and were often taught by the instructors coordinating the course. From 10 AM to 11:30 AM we would work on the first lab for the day. We would then listen to a guest speaker for about an hour. Between 12:30 PM and 1 PM we would eat lunch. After lunch we would work on lab 2 until approximately 2:45 PM. We would then work on lab 3 unti 5:00 PM when we left. </p>

<h1>The Robot</h1>
<img src="Car_Diagram.png"/>
<p> Chassis: Traxxas Rally 74076 </p>
<img src="Hokuyo.png"/>
<p> 1/10 Scale RC car
    4 wheel drive
    Front Wheel Steering (Ackermann)
Processor:
    Nvidia jetson TX1
    Ubuntu for ARM
    Dual band Wi-Fi
2D LIDAR (Light Imaging, Detection and Ranging):
    -Hokuyo UST-10LX
    -Range: 0.06m to ~10m
    -Accuracy: +/-40mm
    -Scan angle of 270 degrees with 0.25 degree resolution (1081 steps)
    -Scen speed: 25ms (40Hz)
    -Ethernet communication, 12VDC 
    A 3D Lidar is basically a set of 2D lidars stacked on top of each other.

IMU (Inertial Measurement Unit)
</p>
<img src="IMU.png"/>
<p>
-”Sparkfun 9 DOF “Razor”
        -MicroElectroMechanical (MEMS) three-axis accelerometer
        -MicroElectroMechanical (MEMS) three-axis gyroscope
        -MicroElectroMechanical (MEMS) three-axis AnisotropicMagnetoResistance (AMR) magnetometer

    IMUs are used to detect the vehicle's translational and angular acceleration (and thus can track translational and angular position and velocity). Uses accelerometers and gyroscopes. Some IMU’s also use magnetometers to detect magnetic fields.

Passive Stereo Camera
Sensorlabs ZED stereo camera
Automatic (embedded) depth perception from .7 to 20 meters
110 degree FOV (Field of View)
4 videos modes
4416 x 1242 @ 15fps
3840 x 1080 @ 30fps
2560 x 720 @ 60 fps
1344 x 376 @ 100 fps
</p>
<img src="http://myzharbot.robot-home.it/blog/software/configuration-nvidia-jetson-tk1/stereolabs-zed-stereo-camera-the-official-getting-started-guide/"/>
<p>Active Stereo Camera
Occipital Structure Sensor
FOV
</p>
<img src="Racecar_Pic2.png"/>
<h1>Week-by-Week Overview</h1>
<p>     For the first three weeks, there was an overarching theme followed by a competition at the end of the week that tested how well we learned and implemented that theme into our code. The fourth week consisted of consolidating everything we learned and all of the code we developed from the previous weeks. The goal for the week was to showcase what we learned to the public through multiple challenges, including image detection as well as wall and obstacle avoidance.
 Below is a week by week overview of what we did in this course. </p>
<h1> Week 1 </h1>
<h2> Technical Goals </h2>
<p>     The theme for the first week was using the vehicle's laser scanner (called a LIDAR) to detect a nearby wall and follow it without hitting it. In this week we were given lectures about the hardware of our robot as well as the platform that we developed our code on, called the Robot Operating System (ROS). ROS is a platform used by academic roboticists to develop robots.
We also received lectures on control systems, with an emphasis on the the PID controller (proportional–integral–derivative controller). </p>
<img src="WallFollowDiagram.png" />
<p> Source: Thursday Laboratory Exercise 1:
Bang-Bang Control System Implementation </p>
<h2> ROS </h2>
<p> ROS was started at the Stanford Artificial Intelligence Laboratory. ROS is a collection of software that provides tools that are similar to that of operating systems. These tools include hardware abstraction, low-level device control, message-passing between processes (nodes), and package management. Processes can be viewed in a graph where one process may be receiving messages from many other processes and be sending messages to others. </p>
<img src="http://robohub.org/wp-content/uploads/2014/01/ros101-3.png" />
<p> Source: http://robohub.org/wp-content/uploads/2014/01/ros101-3.png </p>
<img src="tree_map.png"/>
<p> source: http://library.isr.ist.utl.pt/docs/roswiki/attachments/image_transport(2f)Tutorials(2f)ExaminingImagePublisherSubscriber/transport_graph_with_compressed.png </p>
















