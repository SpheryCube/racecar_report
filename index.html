<!DOCTYPE html>
<html>
<body>

<h1>Introduction and Project Overview </h1>
<p>    This page is dedicated to explaining what I have done and learned through Beaver Works’ 2016 summer program on autonomous vehicles. Beaver Works is a program that is run jointly through the Massachusetts Institute of Technology’s Engineering School and the nearby Lincoln Laboratory. Beaver Works focuses on creating project based learning and educational programs. This year was the first year that Beaver Works provided a summer program for rising high school seniors.
    As previously mentioned, this year’s Beaver Works’ Summer Program focused on the the programming and software that goes into making an autonomous vehicle. However, in addition to learning how to code an autonomous vehicle, we received daily presentations from guest speakers including people from MIT, Lincoln Laboratory, NASA’s Jet Propulsion Laboratory and an autonomous car manufacturing company in Germany. The subjects of these presentations ranged throughout all of the sciences and engineering disciplines, not just computer science and robotics. On a less technical side, we also attended a communications class twice a week for an hour in order to learn ways to be a more effective team member.
    The program spanned a four week period from July 11th to August 5th. However, there was a lot of pre program material to study so that students could come in with the necessary background knowledge to immediately start programming the race car.
    A normal day consisted of beginning at 9 AM with a technical lecture that related to the labs for the day and/or challenge for the week. These lectures would normally last about an hour and were often taught by the instructors coordinating the course. From 10 AM to 11:30 AM we would work on the first lab for the day. We would then listen to a guest speaker for about an hour. Between 12:30 PM and 1 PM we would eat lunch. After lunch we would work on lab 2 until approximately 2:45 PM. We would then work on lab 3 unti 5:00 PM when we left. </p>

<h1>The Robot</h1>
<img src="Car_Diagram.png"/>
<p> Chassis: Traxxas Rally 74076 </p>
<img src="Hokuyo.png"/>
<p> 1/10 Scale RC car
    4 wheel drive
    Front Wheel Steering (Ackermann)
Processor:
    Nvidia jetson TX1
    Ubuntu for ARM
    Dual band Wi-Fi
2D LIDAR (Light Imaging, Detection and Ranging):
    -Hokuyo UST-10LX
    -Range: 0.06m to ~10m
    -Accuracy: +/-40mm
    -Scan angle of 270 degrees with 0.25 degree resolution (1081 steps)
    -Scen speed: 25ms (40Hz)
    -Ethernet communication, 12VDC 
    A 3D Lidar is basically a set of 2D lidars stacked on top of each other.

IMU (Inertial Measurement Unit)
</p>
<img src="IMU.png"/>
<p>
-”Sparkfun 9 DOF “Razor”
        -MicroElectroMechanical (MEMS) three-axis accelerometer
        -MicroElectroMechanical (MEMS) three-axis gyroscope
        -MicroElectroMechanical (MEMS) three-axis AnisotropicMagnetoResistance (AMR) magnetometer

    IMUs are used to detect the vehicle's translational and angular acceleration (and thus can track translational and angular position and velocity). Uses accelerometers and gyroscopes. Some IMU’s also use magnetometers to detect magnetic fields.

Passive Stereo Camera
Sensorlabs ZED stereo camera
Automatic (embedded) depth perception from .7 to 20 meters
110 degree FOV (Field of View)
4 videos modes
4416 x 1242 @ 15fps
3840 x 1080 @ 30fps
2560 x 720 @ 60 fps
1344 x 376 @ 100 fps
</p>
<img src="http://myzharbot.robot-home.it/blog/software/configuration-nvidia-jetson-tk1/stereolabs-zed-stereo-camera-the-official-getting-started-guide/"/>
<p>Active Stereo Camera
Occipital Structure Sensor
FOV
</p>
<img src="Racecar_Pic2.png"/>
<h1>Week-by-Week Overview</h1>
<p>     For the first three weeks, there was an overarching theme followed by a competition at the end of the week that tested how well we learned and implemented that theme into our code. The fourth week consisted of consolidating everything we learned and all of the code we developed from the previous weeks. The goal for the week was to showcase what we learned to the public through multiple challenges, including image detection as well as wall and obstacle avoidance.
 Below is a week by week overview of what we did in this course. </p>
<h1> Week 1 </h1>
<h2> Technical Goals </h2>
<p>     The theme for the first week was using the vehicle's laser scanner (called a LIDAR) to detect a nearby wall and follow it without hitting it. In this week we were given lectures about the hardware of our robot as well as the platform that we developed our code on, called the Robot Operating System (ROS). ROS is a platform used by academic roboticists to develop robots.
We also received lectures on control systems, with an emphasis on the the PID controller (proportional–integral–derivative controller). </p>
<img src="WallFollowDiagram.png" />
<p> Source: Thursday Laboratory Exercise 1:
Bang-Bang Control System Implementation </p>
<h2> ROS </h2>
<p> ROS was started at the Stanford Artificial Intelligence Laboratory. ROS is a collection of software that provides tools that are similar to that of operating systems. These tools include hardware abstraction, low-level device control, message-passing between processes (nodes), and package management. Processes can be viewed in a graph where one process may be receiving messages from many other processes and be sending messages to others. </p>
<img src="http://robohub.org/wp-content/uploads/2014/01/ros101-3.png" />
<p> Source: http://robohub.org/wp-content/uploads/2014/01/ros101-3.png </p>
<img src="tree_map.png"/>
<p> source: http://library.isr.ist.utl.pt/docs/roswiki/attachments/image_transport(2f)Tutorials(2f)ExaminingImagePublisherSubscriber/transport_graph_with_compressed.png </p>
<p> The above example shows “ROS topics” and “ROS nodes”. ROS topics are places where ROS publishers can publish (send) data to. ROS publishers are said to “talk” to ROS topics. Any node that listens (aka is subscribed to) a ROS topic will receive any data that was published to it. Any given node/program/process that a ROS programmer makes can initialize publishers and subscribers that can publish or subscribe to any number of topics. Each topic can only receive a specific message type (string, Bool, Int32, etc.) </p>

<h2> Control Systems </h2>
<img src="http://www.dfe.com/resources/faq_openloopvsclosedloop.html"/>
<p> Source: http://www.dfe.com/resources/faq_openloopvsclosedloop.html </p>
<img src="https://en.wikipedia.org/wiki/Control_theory#/media/File:Feedback_loop_with_descriptions.svg" />
<p> https://en.wikipedia.org/wiki/Control_theory#/media/File:Feedback_loop_with_descriptions.svg 
PID Controller</p>
<img src="https://en.wikipedia.org/wiki/PID_controller#/media/File:PID_en.svg" />
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1bcb82541c2fd40dd893d3893e04b4d6b1f02330" />
<p> Source: https://en.wikipedia.org/wiki/PID_controller </p>
<p>     The study of control systems is very in depth; so much so that people usually only get PhD’s in certain classes of control systems. There are two main classes of control systems: “open-loop” and “closed loop” control systems. Closed loop control systems use sensors to approximate the state of the system and then send the data from the sensors to the controller so that the controller can adjust the system to a desired state. Errors from the sensors in the state approximation of a closed loop system often send the system into a downward spiral since an error to the controller may result in an erroneous action of the system, leading to an even further sensor errors. This is one of the reasons that a lot of research and mathematics has gone into developing these systems. On the other hand, in open loop systems, there is no feedback from the sensors. Source: https://en.wikipedia.org/wiki/Control_system  </p>

<p>     The PID controller is a specific type of closed loop controller. PID stands for Proportional action, Integral action, and Derivative action. 
    The derivative aspect measures the rate of change of error with respect to time, where the error is the difference between a desired value (aka the “setpoint”) and the value determined by sensor data. If the sensor value approaches the desired value rapidly, the actuator is backed off early so that the error can coast to zero. If the sensor value begins to rapidly move away from the setpoint, then the actuator will work to bring t in proportion to the rate of change in error. For this reason the derivative term in a PID controller is normally negative.
</p>

<h2> Approach </h2>
<p>     Although not in the spirit of ROS, we decided that it would be easier to simply have the PID controller and the wall-following mechanism written in a single node since the challenge was relatively simple and there was no real need to delegate severals nodes to multiple people. However, because it was important for everyone to understand what was going, we all sat around one computer, suggesting ideas to be implemented into the node.
 </p>

<h2>  Process </h2>
<p>     There were several wall following methods that we tried. We made sure that for each wall following method we made, we made it possible to very easily switch which wall (left or right) the vehicle would follow.
    Initially we attempted to use the single-point wall following method. What this meant is that we chose a single angle to use from the lidar scan as a reference to find out how far the wall was away from us and then controlled our position away from the wall using the PID. This method, however, is not particularly robust since the robot may not detect curves in the wall that appear in front of the position of the chosen laser screen.
    Our second attempt involved using a two point wall following method. This method entailed using some basic trigonometry to solve for the distance between the car and the wall. </p>

<h2> Results </h2>
<p> We found that the two point wall following method worked the best.
    In the end, we set our chosen distance from the wall at 0.5 meters. We ended up only using a PD controller, with a P value of [] and a D value of []. (Note: Go to windows, get file)
    In the first week’s final challenge, our car made it to the “Elimination Round” where we ended up losing to car 63. Despite this, we received third of nine in the time trials. </p>

<h1> Week 2 </h1>
<h2> Technical Goals </h2>
<p>     Week 2 focused on image processing and computer vision. In this week we were given lectures on thresholding, convolutions, and custom ROS messages. The end of the week challenge for week 2 was to implement a program/programs that would allow the vehicle to differentiate a green sheet of paper from a red sheet of paper and then turn left or right depending on which color it saw. After turning, it would proceed to performing wall following on either a left or right wall (which again was dependent on the color of the paper).  </p>
<h3> Image Processing and Blob Detection </h3>
<p>     A blob is defined as a region of an image where the pixels have some property or properties that are constant. There are two different type of blob detectors: differential methods and methods based on local extrema, aka, interest region operators. </p>

<h2> Approach </h2>
<p>     My group decided to break the task up into three main nodes in addition to the emergency stop node. The architecture for our project looked similar to this: </p>
<img src="week_2_architecture.png" />
<p>     In order to launch all of the nodes at once, we created a ros launch file. Ros launch files are a powerful tool that allow for multiple nodes to be started with a single command, allow parameters to be set, and allow for rostopic names to be changed so that rostopic naming conflicts between the set of nodes being launched can be mitigated. </p>
<h2> Process </h2>
<p>
The separate nodes that we decided to split our project up into allowed our group to split up the work among multiple people. It also gave us practice using ROS topics, subscribers, and publishers. 
</p>
<p>
We had some difficulty adjusting our blob detector’s parameters to find the blobs. We also realized the changes in room lighting can affect whether or not the program would detect the blob. In order to find a better range of colors to threshold over, we took a picture of the blob we were trying to detect in the lighting that would be used in the end of the week challenge. We then used a photo editor (gimp) with the image to determine the specific HSV values for the pixels of the blob. We could then edit our blob detector’s parameters accordingly. This method ended up being successful. 
</p>
<p>
Below is the picture we used to find the HSV values of the blob:
</p>
<img src="blob_w2.png" />
<p>
    The image above also shows the square in which the vehicle is required to make its turn in, ensuring that the vehicle is adequately close to the blob.  The image also shows how the walls are curved, thus testing the robustness of our wall follower.
</p>
<h2> Results </h2>
<p>    Our vehicle sadly was not able to complete the challenge. However, we feel like this was only because we ran out of time to adjust the constants in our code, not because we had major conceptual errors or because we couldn’t implement the concepts correctly.
</p>



























